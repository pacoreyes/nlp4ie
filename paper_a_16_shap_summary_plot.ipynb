{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-09T08:10:30.014913Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "# Disable upper limit for MPS memory allocations\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "from pprint import pprint\n",
    "import shap\n",
    "import torch\n",
    "# import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from lib.utils import load_jsonl_file\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "CLASS_NAMES = ['monologic', 'dialogic']\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# Load dataset\n",
    "DATASET = load_jsonl_file(\"shared_data/dataset_1_6_1b_test.jsonl\")\n",
    "\n",
    "\n",
    "def get_device():\n",
    "  \"\"\"Returns the appropriate device available in the system: CUDA, MPS, or CPU\"\"\"\n",
    "  \"\"\"if torch.backends.mps.is_available():\n",
    "    return torch.device(\"mps\")\n",
    "  elif torch.cuda.is_available():\n",
    "    return torch.device(\"cuda\")\n",
    "  else:\n",
    "    return torch.device(\"cpu\")\"\"\"\n",
    "  return torch.device(\"cpu\")\n",
    "  \n",
    "\n",
    "# Set device\n",
    "device = get_device()\n",
    "print(f\"\\nUsing device: {str(device).upper()}\\n\")\n",
    "\n",
    "# Initialize constants\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "MODEL_PATH = 'models/1/paper_a_x_dl_bert_train_hop_bert.pth'\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                      num_labels=len(CLASS_NAMES),\n",
    "                                                      hidden_dropout_prob=0.1)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# split dataset in both classes\n",
    "monologic_texts = [d for d in DATASET if d['label'] == \"monologic\"]\n",
    "dialogic_texts = [d for d in DATASET if d['label'] == \"dialogic\"]\n",
    "\n",
    "# Prepare the background data\n",
    "background_data = monologic_texts[:2] + dialogic_texts[:2]\n",
    "\n",
    "# Prepare the texts to generate predictions\n",
    "texts = monologic_texts[100:101] + dialogic_texts[100:101]\n",
    "\n",
    "# Extract texts for background data\n",
    "background_texts = [d['text'] for d in background_data]\n",
    "# print(len(background_texts))\n",
    "\n",
    "# Extract texts for the texts variable\n",
    "texts_to_analyze = [d['text'] for d in texts]\n",
    "# print(len(texts_to_analyze))\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(texts_to_analyze, padding='max_length', truncation=True, return_tensors=\"pt\",\n",
    "                   max_length=MAX_LENGTH)\n",
    "\n",
    "# Move inputs to the same device as your model\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "\n",
    "def model_wrapper(x):\n",
    "    # Convert the SHAP input (NumPy array) to torch tensor\n",
    "    input_ids = torch.tensor(x).long().to(device)\n",
    "    \n",
    "    # Generate attention masks based on input_ids: 1 for tokens, 0 for padding\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long().to(device)\n",
    "    \n",
    "    # Apply the model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.softmax(output.logits, dim=-1)\n",
    "    \n",
    "    # Convert the probabilities to NumPy array and return\n",
    "    return probabilities.cpu().numpy()\n",
    "\n",
    "# Tokenize the background data texts for SHAP\n",
    "background_inputs = tokenizer(background_texts, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=MAX_LENGTH)\n",
    "background_input_ids = background_inputs['input_ids'].to(device)\n",
    "\n",
    "# Convert background input IDs to numpy for SHAP Explainer initialization\n",
    "background = background_input_ids.detach().cpu().numpy()\n",
    "\n",
    "# Initialize the SHAP Explainer\n",
    "explainer = shap.Explainer(model_wrapper, background)\n",
    "\n",
    "# Generate SHAP values for the input texts\n",
    "shap_values = explainer(input_ids.detach().cpu().numpy(), max_evals=1024, batch_size=8)\n",
    "\n",
    "# Plot the SHAP summary plot\n",
    "shap.summary_plot(shap_values, features=inputs['input_ids'].detach().cpu().numpy(), feature_names=tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from shared_data/dataset_1_6_1b_test.jsonl...\n",
      "Loaded 468 items.\n",
      "\n",
      "Using device: CPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "140be207c1abff56",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
